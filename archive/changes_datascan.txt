Changes:
Hank Hopkins 2/18/20

-- General:
  -- Update initialize.py (initialize_updates.py) to print db where data will go.

- Table structure:
  - New/modified tables:
    - dim_file_filename (set to be re-named...)
      - dim_file_filename tracks all normalized filenames and to which file set they belong.
      - Columns:
        - id (auto-inc), stdz_file_name (name tbd), file_set_id, client_id, source_id
          - [Idea: file_set_id can also be used as a separate view ?]
          - file_set is the key:
            - if file_set_id is not null,
              then a file is said to belong to a user-authorized set of files, which can be confidently used
              to validate data inventory, namely, presence of expected files.
            - Note: file_set_id should auto-increment per occurrence of unique client_id and source_id ...
        - Note: A file can have multiple rows if it belongs to multiple file sets per its client/source.
    - fact_datascan_refresh:
      - f_d_r is a table which keeps track of which set of files was used for which refresh iteration of datascan per client/source pair.
      - Also keeps track of whether the datascan for that refresh has received all expected files.
        - If the file set variable is null, this indicates un-expected data inventory
      - Columns:
        - id (auto_inc), client_id, source_id, file_set_id, as_expected
      - There will exist only one row per refresh_id per client and source.
      - Refresh id will be dependent on dim_data_load, to be explained.
  - Application and Integration of file_set_id:
    - file_set_id will need to be manually maintained. The datascan and portal will point out when a set of received files does not match a previously defined set.
      - If the received files used in a datascan-refresh are not within a previously existing file_set, file_set_id will default to null; if more files are
        appended to a datascan-refresh to fulfill the expected set of files, then fact_datascan, fact_datascan_refresh, and the portal will update accordingly to
        indicate that the expected set of files was received.
      - If the expected full set of files per refresh changes, one would need to update dim_file_filename to reflect this change.



A1. Update datascan to point to portal_test2 rather than client databases.
-- Idea: Have optional argument to point to individual client db for datascan testing.
  -- 'Published' datascans should be pointed to portal_test2

A1. Get d-scan to successfully import to portal_test2.
1. Update client config JSON to point to portal_test2
  -- Add client id to client config from portal_test2
  -- Add source id to client config from portal_test2
  -- Use portal_test2 as db instead of client-specific db
2. Update Initialize.py (initialize_updates.py)
  -- add client_id to DEFAULT CONFIG
  -- Modify set_config fxn
    -- Add block for client id.
  -- Modify con_db fxn
    -- change db_name from  {}_staging to portal_test2
3. Update issue_tracker.py (issue_tracker_updates.py)
  -- It has a configuration for the table names and population method that needs to be altered to match portal_test2
    -- Add client id
  :
  # Individual client fact_datascan structure:

    # `id` int(8) NOT NULL AUTO_INCREMENT,
    # `file` varchar(150) DEFAULT NULL,
    # `file_id` int(8) DEFAULT NULL,
    # `table_name` varchar(150) DEFAULT NULL,
    # `run_date` varchar(25) DEFAULT NULL,
    # `refresh` varchar(6) DEFAULT NULL,
    # `old_count` int(11) DEFAULT NULL,
    # `new_count` int(11) DEFAULT NULL,
    # `variance` float DEFAULT NULL,
    # `issue_id` int(8) DEFAULT NULL,
    # `message` varchar(300) DEFAULT NULL,
    # `source_id` int(11) DEFAULT NULL,
    # `last_updated_date` varchar(25) DEFAULT NULL,

  # portal_test2 fact_datascan structure:

    # `id` int(8) NOT NULL AUTO_INCREMENT,
    # `file_id` int(8) DEFAULT NULL,
    # `table_name` varchar(150) DEFAULT NULL,
    # `run_date` datetime DEFAULT NULL,
    # `refresh` varchar(8) DEFAULT NULL,
    # `old_count` int(11) DEFAULT NULL,
    # `new_count` int(11) DEFAULT NULL,
    # `variance` float DEFAULT NULL,
    # `issue_id` int(8) DEFAULT NULL,
    # `message` varchar(300) DEFAULT NULL,
    # `source_id` int(11) DEFAULT NULL,
    # `last_updated_date` varchar(25) DEFAULT NULL,
    # `file` varchar(11) DEFAULT NULL,
    # `field` longblob,
    # `row_count` int(11) DEFAULT NULL,
    # `client_id` int(11) unsigned DEFAULT NULL,
    # `column_count` int(11) DEFAULT NULL,
    # `previous_date_of_receipt` datetime DEFAULT NULL,
    # `data_load_id` int(11) DEFAULT NULL,

  4. Update out_file.py (out_file_updates.py)
    -- This script passes arguments to the issue tracker.
    -- Must align with format in issue tracker.

A1 is finished. 2/21/20

A2. Create methodology for implementing standardized filenames. Began: 3/2/20
- There will exist a folder within the datascan folder with each client's normalization method.
  - There is bound to be some overlap
  - A config would be good.
  - Will need to handle odd file types, such as excel or zips.
- Created normalize_filenames.py script.
  - This script uses the FILE_NORMAL_CONFIG.json to find client and which normalization script to run.
    - fxn standardize_files returns the normalized filenames in a list
  - Also, looks in dim_file_filename for existence of nml file name
    - If normalized filename does not exist

A3. Develop methodology for matching files and previous scan results. Began 3/3/20
- Overview of matching process:
  - First, the datascan will attempt to standardize filenames. Then, it will see if this set of files has been designated as an expected set of files, in the past.
    - If the set of files exists:
      - Look in fact_datascan and fact_datascan_refresh for the latest/maximum refresh_id where this set of files was past utilized.
    - If set of files dont exist:
      - Look for match for each particular file:
        - use last file match where refresh id is maximum
        - If refresh id is null, look for most recent file match

A4. Implement new issue in dim_issue which specifies that a file has not been matched with an existing normalized filename.
- Stored procedure for checking datascan results for unmatched filenames and once updated, updates and/or deletes record.

A5. Change the importing modules method.
- Currently, the datascan script does this:
  -- Import a, b from A
  -- Import c, d from B ...
      cat = a(dog)
      llama = c(cat)
- It makes more sense to do this:
  -- Import A
  -- Import B
      cat = A.a(dog)
      llama = B.c(cat)
- According to Stack exchange, the two import methods have little to no performance differences.
  - The best reason for change is to have the script the function is from right in your face instead of having to scroll up to the top of the entire script every time.
    - The datascan uses many, many helper scripts and having to scroll up and lose your spot and all that junk is a pain in the butt.
A5 is finished 3/3/20

A3. Implement individual client datascan argument.
